#!/usr/bin/env python

"""
Using convnets to classify EEG images into four categories corresponding to four
WM load levels.
Implementation using Lasagne module.
Input images are scaled by divining by 256. No baseline correction.
EEG images are generated by extracting FFT power within theta, alpha and beta frequency bands
averaged over whole trial duration (3.5 sec).
"""

from __future__ import print_function

import sys
import os
import time

import numpy as np
np.random.seed(1234)
import scipy.io
import theano
import theano.tensor as T

import lasagne
from lasagne.layers import InputLayer, DenseLayer, NonlinearityLayer
# from lasagne.layers.dnn import Conv2DDNNLayer as ConvLayer
from lasagne.layers import Conv2DLayer as ConvLayer
from lasagne.layers import DropoutLayer
from lasagne.layers import Pool2DLayer as PoolLayer
from lasagne.nonlinearities import softmax

from utils import load_data, reformatInput

augment = True      # Augment data
filename = 'EEG_images'
filename_aug = 'EEG_images_aug'
subjectsFilename = 'trials_subNums'
model = 'cnn'
num_epochs = 300

batch_size = 20
nb_classes = 4


# ##################### Build the neural network model #######################
# This script supports three types of models. For each one, we define a
# function that takes a Theano variable representing the input and returns
# the output layer of a neural network model built in Lasagne.

def build_cnn(input_var=None):
    # As a third model, we'll create a CNN of two convolution + pooling stages
    # and a fully-connected hidden layer in front of the output layer.

    # Input layer, as usual:
    network = lasagne.layers.InputLayer(shape=(None, 3, 40, 40),
                                        input_var=input_var)
    # This time we do not apply input dropout, as it tends to work less well
    # for convolutional layers.

    # Convolutional layer with 32 kernels of size 5x5. Strided and padded
    # convolutions are supported as well; see the docstring.
    network = lasagne.layers.Conv2DLayer(
            network, num_filters=64, filter_size=(3, 3),
            W=lasagne.init.GlorotUniform())
    network = lasagne.layers.Conv2DLayer(
            network, num_filters=64, filter_size=(3, 3),
            W=lasagne.init.GlorotUniform())
    # network = lasagne.layers.Conv2DLayer(
    #         network, num_filters=40, filter_size=(3, 3),
    #         W=lasagne.init.GlorotUniform())
    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))
    # Expert note: Lasagne provides alternative convolutional layers that
    # override Theano's choice of which implementation to use; for details
    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.

    network = lasagne.layers.Conv2DLayer(
            network, num_filters=128, filter_size=(3, 3))
    network = lasagne.layers.Conv2DLayer(
            network, num_filters=128, filter_size=(3, 3))
    # network = lasagne.layers.Conv2DLayer(
    #         network, num_filters=80, filter_size=(3, 3))
    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))

    # A fully-connected layer of 256 units with 50% dropout on its inputs:
    network = lasagne.layers.DenseLayer(
            lasagne.layers.dropout(network, p=.5),
            num_units=512,
            nonlinearity=lasagne.nonlinearities.rectify)

    # And, finally, the 10-unit output layer with 50% dropout on its inputs:
    network = lasagne.layers.DenseLayer(
            lasagne.layers.dropout(network, p=.5),
            num_units=4,
            nonlinearity=lasagne.nonlinearities.softmax)

    return network

def build_cnn2(input_var=None):
    net = {}
    net['input'] = InputLayer((None, 3, 40, 40), input_var=input_var)
    net['conv1_1'] = ConvLayer(net['input'], 40, 3, pad='full', nonlinearity=lasagne.nonlinearities.rectify)
    # net['conv1_2'] = ConvLayer(net['conv1_1'], 40, 3, pad='full', nonlinearity=lasagne.nonlinearities.rectify)
    net['pool1'] = PoolLayer(net['conv1_1'], 2)
    net['drop1'] = DropoutLayer(net['pool1'], p=0.25)
    net['conv2_1'] = ConvLayer(net['drop1'], 80, 3, pad='full', nonlinearity=lasagne.nonlinearities.rectify)
    # net['conv2_2'] = ConvLayer(net['conv2_1'], 80, 3, pad='full', nonlinearity=lasagne.nonlinearities.rectify)
    net['pool2'] = PoolLayer(net['conv2_1'], 2)
    net['drop2'] = DropoutLayer(net['pool2'], p=0.25)
    net['fc1'] = DenseLayer(net['drop2'], num_units=512, nonlinearity=lasagne.nonlinearities.rectify)
    net['drop3'] = DropoutLayer(net['fc1'], p=0.5)
    net['fc2'] = DenseLayer(net['drop3'], num_units=nb_classes, nonlinearity=softmax)

    return net['fc2']


# ############################# Batch iterator ###############################
# This is just a simple helper function iterating over training data in
# mini-batches of a particular size, optionally in random order. It assumes
# data is available as numpy arrays. For big datasets, you could load numpy
# arrays as memory-mapped files (np.load(..., mmap_mode='r')), or write your
# own custom data iteration function. For small datasets, you can also copy
# them to GPU at once for slightly improved performance. This would involve
# several changes in the main program, though, and is not demonstrated here.

def iterate_minibatches(inputs, targets, batchsize, shuffle=False):
    assert len(inputs) == len(targets)
    if shuffle:
        indices = np.arange(len(inputs))
        np.random.shuffle(indices)
    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):
        if shuffle:
            excerpt = indices[start_idx:start_idx + batchsize]
        else:
            excerpt = slice(start_idx, start_idx + batchsize)
        yield inputs[excerpt], targets[excerpt]


# ############################## Main program ################################
# Everything else will be handled in our main program now. We could pull out
# more functions to better separate the code, but it wouldn't make it any
# easier to read.
def main():
    # Load the dataset
    print("Loading data...")
    data, labels = load_data(filename)
    mat = scipy.io.loadmat(subjectsFilename, mat_dtype=True)
    subjNumbers = np.squeeze(mat['subjectNum'])     # subject IDs for each trial

    if augment:
        data_aug, labels_aug = load_data(filename_aug)
        data = np.vstack((data, data_aug))
        labels = np.vstack((labels, labels_aug))
        subjNumbers = np.concatenate((subjNumbers, subjNumbers))

    # Leave-Subject-Out cross validation
    fold_pairs = []
    for i in np.unique(subjNumbers):
        ts = subjNumbers == i
        tr = np.squeeze(np.nonzero(np.bitwise_not(ts)))
        np.random.shuffle(ts)       # Shuffle indices
        np.random.shuffle(tr)
        fold_pairs.append((tr, np.squeeze(np.nonzero(ts))))

    validScores, testScores = [], []
    for foldNum, fold in enumerate(fold_pairs):
        print('Beginning fold {0} out of {1}'.format(foldNum+1, len(fold_pairs)))
        (X_train, y_train), (X_val, y_val), (X_test, y_test) = reformatInput(data, labels, fold)
        X_train = X_train.astype("float32", casting='unsafe')
        X_val = X_val.astype("float32", casting='unsafe')
        X_test = X_test.astype("float32", casting='unsafe')
        # Normalizing the input
        # X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train.flatten(), axis=0)
        # X_val = (X_val - np.mean(X_val, axis=0)) / np.std(X_val.flatten(), axis=0)
        # X_test = (X_test - np.mean(X_test, axis=0)) / np.std(X_test.flatten(), axis=0)
        # X_train = (X_train - np.mean(X_train, axis=0)) / np.float32(256)
        # X_val = (X_val - np.mean(X_train, axis=0)) / np.float32(256)
        # X_test = (X_test - np.mean(X_train, axis=0)) / np.float32(256)
        X_train = X_train / np.float32(256)
        X_val = X_val / np.float32(256)
        X_test = X_test / np.float32(256)

        # Prepare Theano variables for inputs and targets
        input_var = T.tensor4('inputs')
        target_var = T.ivector('targets')

        # Create neural network model (depending on first command line parameter)
        print("Building model and compiling functions...")
        network = build_cnn(input_var)

        # Create a loss expression for training, i.e., a scalar objective we want
        # to minimize (for our multi-class problem, it is the cross-entropy loss):
        prediction = lasagne.layers.get_output(network)
        loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)
        loss = loss.mean()
        # We could add some weight decay as well here, see lasagne.regularization.

        # Create update expressions for training, i.e., how to modify the
        # parameters at each training step. Here, we'll use Stochastic Gradient
        # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.
        params = lasagne.layers.get_all_params(network, trainable=True)
        updates = lasagne.updates.nesterov_momentum(
                loss, params, learning_rate=0.001, momentum=0.9)

        # Create a loss expression for validation/testing. The crucial difference
        # here is that we do a deterministic forward pass through the network,
        # disabling dropout layers.
        test_prediction = lasagne.layers.get_output(network, deterministic=True)
        test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,
                                                                target_var)
        test_loss = test_loss.mean()
        # As a bonus, also create an expression for the classification accuracy:
        test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),
                          dtype=theano.config.floatX)

        # Compile a function performing a training step on a mini-batch (by giving
        # the updates dictionary) and returning the corresponding training loss:
        train_fn = theano.function([input_var, target_var], loss, updates=updates)

        # Compile a second function computing the validation loss and accuracy:
        val_fn = theano.function([input_var, target_var], [test_loss, test_acc])

        # Finally, launch the training loop.
        print("Starting training...")
        best_validation_accu = 0
        # We iterate over epochs:
        for epoch in range(num_epochs):
            # In each epoch, we do a full pass over the training data:
            train_err = 0
            train_batches = 0
            start_time = time.time()
            for batch in iterate_minibatches(X_train, y_train, batch_size, shuffle=False):
                inputs, targets = batch
                train_err += train_fn(inputs, targets)
                train_batches += 1

            # And a full pass over the validation data:
            val_err = 0
            val_acc = 0
            val_batches = 0
            for batch in iterate_minibatches(X_val, y_val, batch_size, shuffle=False):
                inputs, targets = batch
                err, acc = val_fn(inputs, targets)
                val_err += err
                val_acc += acc
                val_batches += 1
            av_train_err = train_err / train_batches
            av_val_err = val_err / val_batches
            av_val_acc = val_acc / val_batches
            # Then we print the results for this epoch:
            print("Epoch {} of {} took {:.3f}s".format(
                epoch + 1, num_epochs, time.time() - start_time))
            print("  training loss:\t\t{:.6f}".format(av_train_err))
            print("  validation loss:\t\t{:.6f}".format(av_val_err))
            print("  validation accuracy:\t\t{:.2f} %".format(av_val_acc * 100))
            if av_val_acc > best_validation_accu:
                best_validation_accu = av_val_acc

                # After training, we compute and print the test error:
                test_err = 0
                test_acc = 0
                test_batches = 0
                for batch in iterate_minibatches(X_test, y_test, batch_size, shuffle=False):
                    inputs, targets = batch
                    err, acc = val_fn(inputs, targets)
                    test_err += err
                    test_acc += acc
                    test_batches += 1

                av_test_err = test_err / test_batches
                av_test_acc = test_acc / test_batches
                print("Final results:")
                print("  test loss:\t\t\t{:.6f}".format(av_test_err))
                print("  test accuracy:\t\t{:.2f} %".format(av_test_acc * 100))
                # Dump the network weights to a file like this:
                np.savez('weigths_lasg{0}'.format(foldNum), *lasagne.layers.get_all_param_values(network))
        validScores.append(best_validation_accu * 100)
        testScores.append(av_test_acc * 100)
        print('-'*50)
        print("Best validation accuracy:\t\t{:.2f} %".format(best_validation_accu * 100))
        print("Best test accuracy:\t\t{:.2f} %".format(av_test_acc * 100))
    scipy.io.savemat('cnn_lasg_results', {'validAccu': validScores, 'testAccu': testScores})
        #
        # And load them again later on like this:
        # with np.load('model.npz') as f:
        #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]
        # lasagne.layers.set_all_param_values(network, param_values)

if __name__ == '__main__':
    main()